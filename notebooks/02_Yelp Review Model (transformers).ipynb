{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26269aea-b362-421b-a64d-0a67fae63f14",
   "metadata": {},
   "source": [
    "## Part Three: Fine-tuning a BERT neural network model based on the Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba103dc4-314a-482b-b111-2a94ad2cb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Environment Summary\n",
      "----------------------------------------\n",
      "                     Timestamp: 2025-04-23T19:19:50.537793\n",
      "                      Hostname: autodl-container-ddce40b55a-be393d15\n",
      "                            OS: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\n",
      "                Python Version: 3.12.3\n",
      "                 Torch Version: 2.5.1+cu124\n",
      "                CUDA Available: True\n",
      "                  CUDA Version: 12.4\n",
      "                  Device Count: 2\n",
      "                Current Device: 0\n",
      "                   Device Name: NVIDIA GeForce RTX 4090 D\n",
      "          Transformers Version: 4.51.3\n",
      "               Total CPU Cores: 96\n",
      "                Total RAM (GB): 1081.83\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import platform\n",
    "import psutil\n",
    "import datetime\n",
    "import socket\n",
    "\n",
    "config_summary = {\n",
    "    \"Timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"Hostname\": socket.gethostname(),\n",
    "    \"OS\": platform.platform(),\n",
    "    \"Python Version\": platform.python_version(),\n",
    "    \"Torch Version\": torch.__version__,\n",
    "    \"CUDA Available\": torch.cuda.is_available(),\n",
    "    \"CUDA Version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "    \"Device Count\": torch.cuda.device_count(),\n",
    "    \"Current Device\": torch.cuda.current_device() if torch.cuda.is_available() else \"N/A\",\n",
    "    \"Device Name\": torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"N/A\",\n",
    "    \"Transformers Version\": transformers.__version__,\n",
    "    \"Total CPU Cores\": psutil.cpu_count(logical=False),\n",
    "    \"Total RAM (GB)\": round(psutil.virtual_memory().total / 1e9, 2),\n",
    "}\n",
    "\n",
    "print(\"Training Environment Summary\\n\" + \"-\"*40)\n",
    "for k, v in config_summary.items():\n",
    "    print(f\"{k:>30}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633530ed-c0b8-453b-b7ef-ade530a21414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 19:19:52.783610: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 19:19:52.796687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745407192.811108    2100 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745407192.815276    2100 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745407192.827097    2100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745407192.827113    2100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745407192.827114    2100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745407192.827115    2100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-23 19:19:52.831512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d79f72a-772a-4cbf-a04a-6811cd4c91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.read_json('../database/yelp_academic_dataset_business.json', lines=True)\n",
    "review_df = pd.read_json('../database/yelp_academic_dataset_review.json', lines=True)\n",
    "restaurants = business_df[business_df['categories'].str.contains('Restaurant', na=False)]\n",
    "restaurant_reviews = review_df[review_df['business_id'].isin(restaurants['business_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83e3111-4afc-4510-b1a9-b023c7895f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(restaurant_reviews))\n",
    "train = restaurant_reviews.iloc[:train_size][['text', 'stars']]\n",
    "test = restaurant_reviews.iloc[train_size:][['text', 'stars']]\n",
    "\n",
    "text_train = train['text'].values\n",
    "labels_train = train['stars'].values.astype(int) - 1\n",
    "text_test = test['text'].values\n",
    "labels_test = test['stars'].values.astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c672a5-dd76-4bbe-a894-9399851e876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DistilBERT tokenizer with specified configuration\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    '../assets/distilbert-base-uncased',   # Use the uncased DistilBERT base model\n",
    "    do_lower_case=True                     # Convert input text to lowercase\n",
    ")\n",
    "\n",
    "text_train = train.text.values\n",
    "labels_train = train.stars.values\n",
    "text_test = test.text.values\n",
    "labels_test = test.stars.values\n",
    "\n",
    "# Store token IDs\n",
    "token_id_train = []\n",
    "token_id_test = []\n",
    "\n",
    "# Store attention masks\n",
    "attention_masks_train = []\n",
    "attention_masks_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd203be5-c453-47b4-914c-8072a3027b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3307278/3307278 [31:24<00:00, 1755.33it/s] \n"
     ]
    }
   ],
   "source": [
    "# Encode each text sample in the training set\n",
    "for sample in tqdm(text_train):\n",
    "    encoding_dict = Bert_preprocess(sample, tokenizer)\n",
    "    token_id_train.append(encoding_dict['input_ids'])\n",
    "    attention_masks_train.append(encoding_dict['attention_mask'])\n",
    "\n",
    "# Concatenate all token IDs and attention masks in the list into single tensors\n",
    "token_id_train = torch.cat(token_id_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dedef1c8-b3c3-4f8f-ac7e-73d5b0145a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1417406/1417406 [13:41<00:00, 1724.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode each text sample in the test set\n",
    "for sample in tqdm(text_test):\n",
    "    encoding_dict = Bert_preprocess(sample, tokenizer)\n",
    "    token_id_test.append(encoding_dict['input_ids'])\n",
    "    attention_masks_test.append(encoding_dict['attention_mask'])\n",
    "\n",
    "# Concatenate all token IDs and attention masks into single tensors\n",
    "token_id_test = torch.cat(token_id_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef6220-16f5-49d5-b2df-f44e2bb2aa49",
   "metadata": {},
   "source": [
    "This step takes a long time, so we save the results now for later use to avoid reprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d423fb-0eb4-4cb1-8081-50da3962bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save token IDs and attention masks\n",
    "torch.save(token_id_train, '../assets/tokenization/train_token_id.pt')\n",
    "torch.save(attention_masks_train, '../assets/tokenization/train_attention_masks.pt')\n",
    "torch.save(token_id_test, '../assets/tokenization/test_token_id.pt')\n",
    "torch.save(attention_masks_test, '../assets/tokenization/test_attention_masks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345bac1a-6447-4d1c-9d9d-2642de4ad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The following code may raise a FutureWarning when loading the .pt file.\n",
    "# The warning message indicates that torch.load() is being used with the default parameter weights_only=False,\n",
    "# which enables pickle-based deserialization and may pose a security risk (e.g., loading malicious code).\n",
    "# This warning is issued by PyTorch to inform users of an upcoming default behavior change, and it does not affect current execution.\n",
    "# To completely suppress this warning, add the following lines:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a148bc68-e5ae-4d3c-a7a4-fcf6d3440743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload the saved token IDs and attention masks\n",
    "# token_id_train = torch.load('../assets/tokenization/train_token_id.pt')\n",
    "# attention_masks_train = torch.load('../assets/tokenization/train_attention_masks.pt')\n",
    "# token_id_test = torch.load('../assets/tokenization/test_token_id.pt')\n",
    "# attention_masks_test = torch.load('../assets/tokenization/test_attention_masks.pt')\n",
    "\n",
    "# # Verify that the data has been loaded correctly\n",
    "# print(\"Training Token IDs:\", token_id_train.shape)\n",
    "# print(\"Training Attention Masks:\", attention_masks_train.shape)\n",
    "# print(\"Testing Token IDs:\", token_id_test.shape)\n",
    "# print(\"Testing Attention Masks:\", attention_masks_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f21ae34-c439-4a67-84cf-f775af77d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels and convert them to torch float type\n",
    "labels_train = F.one_hot(torch.tensor(labels_train), num_classes=6).to(torch.float)\n",
    "labels_test = F.one_hot(torch.tensor(labels_test), num_classes=6).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4e1e1a-8b41-4d01-8922-0d9f34bd66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ../assets/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBertForSequenceClassification model\n",
    "model_distilbert_cls = DistilBertForSequenceClassification.from_pretrained(\n",
    "    '../assets/distilbert-base-uncased',\n",
    "    num_labels=6,                # Set the number of output labels to 6\n",
    "    output_attentions=False,    # Do not output attention weights\n",
    "    output_hidden_states=False, # Do not output hidden states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc95254-bbce-4dcd-9da4-efbbbea0c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(labels_train)),     # Generate an index array equal to the number of training labels\n",
    "    test_size=validation_ratio,       # Set validation set size to 20%\n",
    "    shuffle=True,                     # Shuffle the data before splitting\n",
    "    stratify=labels_train             # Stratify by labels to maintain class distribution\n",
    ")\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train_set = TensorDataset(token_id_train[train_idx],\n",
    "                          attention_masks_train[train_idx],\n",
    "                          labels_train[train_idx])\n",
    "\n",
    "validate_set = TensorDataset(token_id_train[val_idx],\n",
    "                             attention_masks_train[val_idx],\n",
    "                             labels_train[val_idx])\n",
    "\n",
    "test_set = TensorDataset(token_id_test,\n",
    "                         attention_masks_test,\n",
    "                         labels_test)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set,\n",
    "    sampler=RandomSampler(train_set),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validate_set,\n",
    "    sampler=SequentialSampler(validate_set),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_set,\n",
    "    sampler=RandomSampler(test_set),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c463dac-1ad1-4fda-8d68-8310520657a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the optimizer\n",
    "optimizer = torch.optim.AdamW(model_distilbert_cls.parameters(), \n",
    "                              lr=2e-5,         # Learning rate\n",
    "                              eps=1e-08)       # Epsilon for numerical stability\n",
    "\n",
    "# Run on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_distilbert_cls = model_distilbert_cls.to(device)\n",
    "\n",
    "# Recommended number of training epochs according to the paper: 2, 3, or 4\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1dd3ed-8a57-4e01-999c-369b56b15e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [3:56:12<11:48:36, 14172.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Training Loss: 0.1876\n",
      "\t - Validation Accuracy: 0.7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [7:52:35<7:52:37, 14178.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Training Loss: 0.1746\n",
      "\t - Validation Accuracy: 0.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [11:48:45<3:56:14, 14174.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Training Loss: 0.1672\n",
      "\t - Validation Accuracy: 0.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [15:44:58<00:00, 14174.51s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Training Loss: 0.1593\n",
      "\t - Validation Accuracy: 0.7495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in trange(epochs, desc='Epoch'):\n",
    "\n",
    "    # ========== Training Phase ==========\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model_distilbert_cls.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    train_loss = 0\n",
    "    nb_train_examples, nb_train_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_distilbert_cls(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "        loss = outputs['loss']\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_distilbert_cls.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tracking variables\n",
    "        train_loss += loss.item()\n",
    "        nb_train_examples += b_input_ids.size(0)\n",
    "        nb_train_steps += 1\n",
    "\n",
    "    # ========== Validation Phase ==========\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model_distilbert_cls.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    eval_loss = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model_distilbert_cls(b_input_ids,\n",
    "                                        attention_mask=b_input_mask,\n",
    "                                        labels=b_labels)\n",
    "        loss = outputs['loss']\n",
    "        logits = outputs['logits']\n",
    "\n",
    "        eval_loss.append(loss.item())\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Compute validation metrics\n",
    "        batch_accuracy = Bert_compute_batch_accuracy(logits, label_ids)\n",
    "        val_accuracy.append(batch_accuracy)\n",
    "\n",
    "    train_loss_avg = train_loss / nb_train_steps\n",
    "    val_acc_avg = sum(val_accuracy) / len(val_accuracy)\n",
    "\n",
    "    print('\\n\\t - Training Loss: {:.4f}'.format(train_loss_avg))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(val_acc_avg))\n",
    "\n",
    "    with open('training_log.txt', 'a') as f:\n",
    "        f.write('Epoch {}:\\n'.format(_ + 1))\n",
    "        f.write('\\t - Training Loss: {:.4f}\\n'.format(train_loss_avg))\n",
    "        f.write('\\t - Validation Accuracy: {:.4f}\\n\\n'.format(val_acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac6f68-1164-48d3-b6c2-0d2c29217bdc",
   "metadata": {},
   "source": [
    "<table border=\"1\" style=\"text-align: center; border-collapse: collapse;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model Type</th>\n",
    "      <th>Number of Samples</th>\n",
    "      <th>Tokens</th>\n",
    "      <th>Epochs</th>\n",
    "      <th>Learning Rate</th>\n",
    "      <th>Optimizer Decay</th>\n",
    "      <th>Training Loss</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Basic</td>\n",
    "      <td>50,000</td>\n",
    "      <td>256</td>\n",
    "      <td>3</td>\n",
    "      <td>5e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.2157</td>\n",
    "      <td>0.6926</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Basic</td>\n",
    "      <td>100,000</td>\n",
    "      <td>256</td>\n",
    "      <td>3</td>\n",
    "      <td>5e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.2251</td>\n",
    "      <td>0.6970</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Basic</td>\n",
    "      <td>100,000</td>\n",
    "      <td>256</td>\n",
    "      <td>2</td>\n",
    "      <td>2e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.2296</td>\n",
    "      <td>0.7033</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>DistilBERT</td>\n",
    "      <td>50,000</td>\n",
    "      <td>256</td>\n",
    "      <td>3</td>\n",
    "      <td>2e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.1198</td>\n",
    "      <td>0.6770</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>DistilBERT</td>\n",
    "      <td>100,000</td>\n",
    "      <td>256</td>\n",
    "      <td>3</td>\n",
    "      <td>2e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.1949</td>\n",
    "      <td>0.6957</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>DistilBERT</td>\n",
    "      <td>100,000</td>\n",
    "      <td>512</td>\n",
    "      <td>3</td>\n",
    "      <td>2e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.1936</td>\n",
    "      <td>0.7076</td>\n",
    "    </tr>\n",
    "    <tr style=\"font-weight: bold;\">\n",
    "      <td>DistilBERT</td>\n",
    "      <td>200,000</td>\n",
    "      <td>512</td>\n",
    "      <td>4</td>\n",
    "      <td>2e-5</td>\n",
    "      <td>1e-08</td>\n",
    "      <td>0.1616</td>\n",
    "      <td>0.7117</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "533bfcf6-0c8e-40b4-8667-c1e2e1973548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters after completing all training epochs\n",
    "torch.save(model_distilbert_cls.state_dict(), '../assets/weights/model_distilbert_cls.pth')\n",
    "\n",
    "# Save the entire model architecture and weights\n",
    "torch.save(model_distilbert_cls, '../assets/weights/model_distilbert_cls_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39efab04-9977-42f1-b6f5-136e058f2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.18068653614668875\n",
      "Test Accuracy: 0.7500254925422555\n"
     ]
    }
   ],
   "source": [
    "model_distilbert_cls.eval()\n",
    "\n",
    "# Initialize tracking variables for evaluation results\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attention_masks, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_distilbert_cls(b_input_ids, \n",
    "                                    attention_mask=b_attention_masks, \n",
    "                                    labels=b_labels)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs['loss']\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Get predicted logits\n",
    "        logits = outputs['logits']\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Update accuracy tracking\n",
    "        total_eval_accuracy += Bert_compute_batch_accuracy(logits, label_ids)\n",
    "\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n",
    "print(f\"Test Accuracy: {avg_test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
